"""
SPA (Single Page Application) ì‚¬ì´íŠ¸ë¥¼ ìœ„í•œ í†µí•© ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í¼
Claude/Gemini ì§€ì›
"""
import asyncio
import time
from typing import List, Dict, Set, Optional, Any
from playwright.async_api import async_playwright, Page, Browser, TimeoutError as PlaywrightTimeoutError
from dataclasses import dataclass

from src.models.schemas import ProductItem, ScrapingConfig, ScrapingSourceType, SPAConfig
from src.utils.unified_llm_extractor import UnifiedLLMTreatmentExtractor


@dataclass
class SPAScrapingResult:
    url: str
    products: List[ProductItem]
    interactions_performed: int
    content_states: List[str]  # ê° ìƒí˜¸ì‘ìš© í›„ ì½˜í…ì¸  ìƒíƒœ
    error: Optional[str] = None
    processing_time: float = 0.0


class UnifiedSPAContentScraper:
    """SPA ì‚¬ì´íŠ¸ì˜ ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ (Claude/Gemini ì§€ì›)"""

    def __init__(self, config: ScrapingConfig, llm_extractor: UnifiedLLMTreatmentExtractor):
        self.config = config
        self.llm_extractor = llm_extractor
        self.spa_config = config.spa_config
        if not self.spa_config:
            raise ValueError("SPA config is required for SPA scraping")

    async def scrape_spa_content(self, url: str) -> SPAScrapingResult:
        """SPA ì‚¬ì´íŠ¸ì—ì„œ ë™ì  ì½˜í…ì¸ ë¥¼ ìŠ¤í¬ë˜í•‘"""
        start_time = time.time()

        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )

            try:
                page = await browser.new_page()

                # User-Agent ì„¤ì •
                if self.config.headers.get("User-Agent"):
                    await page.set_extra_http_headers({
                        "User-Agent": self.config.headers["User-Agent"]
                    })

                # ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ
                print(f"ğŸŒ í˜ì´ì§€ ë¡œë”©: {url}")
                await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                await page.wait_for_timeout(3000)  # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°

                content_states = []
                all_products = []
                interactions_performed = 0

                # í˜ì´ì§€ ìƒí˜¸ì‘ìš© ìˆ˜í–‰
                for interaction_num in range(self.spa_config.max_interactions):
                    print(f"ğŸ”„ ìƒí˜¸ì‘ìš© {interaction_num + 1}/{self.spa_config.max_interactions}")

                    # í˜„ì¬ ì½˜í…ì¸  ìƒíƒœ ìº¡ì²˜
                    current_content = await page.content()
                    content_hash = str(hash(current_content))

                    # ì¤‘ë³µ ì½˜í…ì¸  ì²´í¬
                    if content_hash in content_states:
                        print(f"âš ï¸  ì¤‘ë³µ ì½˜í…ì¸  ê°ì§€, ìƒí˜¸ì‘ìš© ì¤‘ë‹¨")
                        break

                    content_states.append(content_hash)

                    # ê° interactionì˜ HTMLì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ - ë””ë²„ê¹… ì¶”ê°€
                    try:
                        print(f"ğŸ¤– ìƒí˜¸ì‘ìš© {interaction_num + 1}ì˜ HTML ë…ë¦½ ì²˜ë¦¬ ì¤‘...")
                        print(f"ğŸ“ HTML í¬ê¸°: {len(current_content)} ë¬¸ì")

                        # HTML ìƒ˜í”Œì„ ë¡œê·¸ë¡œ ì €ì¥í•´ì„œ ì‹¤ì œ ë‚´ìš© í™•ì¸
                        from datetime import datetime
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
                        debug_filename = f"data/errors/html_debug_interaction_{interaction_num + 1}_{timestamp}.txt"

                        import os
                        os.makedirs("data/errors", exist_ok=True)
                        with open(debug_filename, "w", encoding="utf-8") as f:
                            f.write(f"=== Interaction {interaction_num + 1} Debug ===\n")
                            f.write(f"HTML í¬ê¸°: {len(current_content)} ë¬¸ì\n")
                            f.write(f"ì‹œê°„: {datetime.now().isoformat()}\n")
                            f.write("=" * 50 + "\n\n")
                            # ì²˜ìŒ 5000ìë§Œ ì €ì¥
                            f.write("HTML ìƒ˜í”Œ (ì²˜ìŒ 5000ì):\n")
                            f.write(current_content[:5000])

                        products = await self.llm_extractor.extract_treatments_from_html_async(
                            current_content, url
                        )
                        if products:
                            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
                            new_products = self._deduplicate_products(all_products, products)
                            all_products.extend(new_products)
                            print(f"âœ… ìƒí˜¸ì‘ìš© {interaction_num + 1}: {len(products)}ê°œ ì¶”ì¶œ â†’ {len(new_products)}ê°œ ì‹ ê·œ (ì´ {len(all_products)}ê°œ)")
                        else:
                            print(f"ğŸ“­ ìƒí˜¸ì‘ìš© {interaction_num + 1}: ìƒˆë¡œìš´ ì œí’ˆ ì—†ìŒ")
                    except Exception as e:
                        print(f"âš ï¸  ìƒí˜¸ì‘ìš© {interaction_num + 1} ì œí’ˆ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")

                    # ë‹¤ìŒ ìƒí˜¸ì‘ìš© ìˆ˜í–‰
                    if interaction_num < self.spa_config.max_interactions - 1:
                        success = await self._perform_interaction(page)
                        if success:
                            interactions_performed += 1
                            # ìƒí˜¸ì‘ìš© í›„ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                            await page.wait_for_timeout(2000)
                        else:
                            print("ğŸ”š ë” ì´ìƒ ìƒí˜¸ì‘ìš©í•  ìš”ì†Œê°€ ì—†ìŒ")
                            break

                processing_time = time.time() - start_time

                return SPAScrapingResult(
                    url=url,
                    products=all_products,
                    interactions_performed=interactions_performed,
                    content_states=content_states,
                    processing_time=processing_time
                )

            except Exception as e:
                processing_time = time.time() - start_time
                return SPAScrapingResult(
                    url=url,
                    products=[],
                    interactions_performed=0,
                    content_states=[],
                    error=str(e),
                    processing_time=processing_time
                )

            finally:
                await browser.close()

    async def _perform_interaction(self, page: Page) -> bool:
        """í˜ì´ì§€ì—ì„œ ìƒí˜¸ì‘ìš© ìˆ˜í–‰"""
        # í´ë¦­ ê°€ëŠ¥í•œ ìš”ì†Œë“¤ì„ ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ì˜
        interaction_selectors = [
            'button:contains("ë”ë³´ê¸°")',
            'button:contains("ë” ë³´ê¸°")',
            'a:contains("ë”ë³´ê¸°")',
            'a:contains("ë” ë³´ê¸°")',
            '.load-more',
            '.btn-more',
            '.more-button',
            '[data-action="load-more"]',
            # í˜ì´ì§€ë„¤ì´ì…˜
            '.pagination .next',
            '.pagination a:last-child',
            '.page-next',
            'a:contains("ë‹¤ìŒ")',
            # ì¼ë°˜ì ì¸ ë²„íŠ¼ë“¤
            'button[type="button"]:visible',
            'a[href="#"]:visible',
        ]

        for selector in interaction_selectors:
            try:
                elements = await page.query_selector_all(selector)
                for element in elements:
                    # ìš”ì†Œê°€ ë³´ì´ê³  í´ë¦­ ê°€ëŠ¥í•œì§€ í™•ì¸
                    if await element.is_visible() and await element.is_enabled():
                        print(f"ğŸ–±ï¸  í´ë¦­: {selector}")
                        await element.click()
                        return True
            except Exception as e:
                continue

        # ìŠ¤í¬ë¡¤ ì‹œë„
        try:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1000)
            return True
        except:
            pass

        return False

    def _deduplicate_products(self, existing_products: List[ProductItem], new_products: List[ProductItem]) -> List[ProductItem]:
        """ì¤‘ë³µ ì œí’ˆ ì œê±°"""
        existing_names = {p.product_name for p in existing_products}
        return [p for p in new_products if p.product_name not in existing_names]


class UnifiedConfigurableScraper:
    """í†µí•© ì„¤ì • ê¸°ë°˜ ìŠ¤í¬ë˜í¼ (Claude/Gemini ì§€ì›)"""

    def __init__(self, config: ScrapingConfig, llm_extractor: UnifiedLLMTreatmentExtractor):
        self.config = config
        self.llm_extractor = llm_extractor

    async def scrape_by_config(self) -> List[ProductItem]:
        """ì„¤ì •ì— ë”°ë¼ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰"""
        all_products = []

        if self.config.source_type == ScrapingSourceType.STATIC_URLS:
            # ì •ì  URL ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ (Promise.all ë°©ì‹)
            print(f"ğŸš€ {len(self.config.static_urls)}ê°œ URL ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

            async def scrape_single_url(url: str) -> List[ProductItem]:
                """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘"""
                try:
                    print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
                    products = await self.llm_extractor.extract_treatments_from_url(url)
                    print(f"âœ… {url}: {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ")
                    return products
                except Exception as e:
                    print(f"âŒ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {url}: {str(e)}")
                    return []

            # ëª¨ë“  URLì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (Promise.allê³¼ ë™ì¼)
            tasks = [scrape_single_url(url) for url in self.config.static_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"âŒ URL {self.config.static_urls[i]} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {str(result)}")
                else:
                    all_products.extend(result)

            print(f"ğŸ‰ ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(all_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

        elif self.config.source_type == ScrapingSourceType.SPA_DYNAMIC:
            # SPA ë™ì  ìŠ¤í¬ë˜í•‘
            if not self.config.spa_config:
                raise ValueError("SPA ìŠ¤í¬ë˜í•‘ì—ëŠ” spa_configê°€ í•„ìš”í•©ë‹ˆë‹¤")

            spa_scraper = UnifiedSPAContentScraper(self.config, self.llm_extractor)

            # ì‹œì‘ URL ê²°ì •
            start_url = self.config.static_urls[0] if self.config.static_urls else self.config.base_url

            result = await spa_scraper.scrape_spa_content(start_url)

            if result.error:
                print(f"âŒ SPA ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {result.error}")
            else:
                all_products.extend(result.products)
                print(f"âœ… SPA ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(result.products)}ê°œ ì œí’ˆ, {result.interactions_performed}ë²ˆ ìƒí˜¸ì‘ìš©")

        return all_products