"""
SPA (Single Page Application) ì‚¬ì´íŠ¸ë¥¼ ìœ„í•œ ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í¼
"""
import asyncio
import time
from typing import List, Dict, Set, Optional, Any
from playwright.async_api import async_playwright, Page, Browser, TimeoutError as PlaywrightTimeoutError
from dataclasses import dataclass

from src.models.schemas import ProductItem, ScrapingConfig, ScrapingSourceType, SPAConfig
from src.utils.llm_extractor import LLMTreatmentExtractor


@dataclass
class SPAScrapingResult:
    url: str
    products: List[ProductItem]
    interactions_performed: int
    content_states: List[str]  # ê° ìƒí˜¸ì‘ìš© í›„ ì½˜í…ì¸  ìƒíƒœ
    error: Optional[str] = None
    processing_time: float = 0.0


class SPAContentScraper:
    """SPA ì‚¬ì´íŠ¸ì˜ ë™ì  ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, config: ScrapingConfig, llm_extractor: LLMTreatmentExtractor):
        self.config = config
        self.llm_extractor = llm_extractor
        self.spa_config = config.spa_config
        if not self.spa_config:
            raise ValueError("SPA config is required for SPA scraping")

    async def scrape_spa_content(self, url: str) -> SPAScrapingResult:
        """SPA ì‚¬ì´íŠ¸ì—ì„œ ë™ì  ì½˜í…ì¸ ë¥¼ ìŠ¤í¬ë˜í•‘"""
        start_time = time.time()

        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )

            try:
                page = await browser.new_page()

                # User-Agent ì„¤ì •
                if self.config.headers.get("User-Agent"):
                    await page.set_extra_http_headers({
                        "User-Agent": self.config.headers["User-Agent"]
                    })

                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(url, wait_until='networkidle')
                await asyncio.sleep(self.spa_config.wait_time)

                # ì´ˆê¸° ëŒ€ê¸° ìš”ì†Œ í™•ì¸
                if self.spa_config.wait_for_element:
                    try:
                        await page.wait_for_selector(
                            self.spa_config.wait_for_element,
                            timeout=15000
                        )
                    except PlaywrightTimeoutError:
                        print(f"âš ï¸ ëŒ€ê¸° ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.spa_config.wait_for_element}")

                # ë™ì  ìƒí˜¸ì‘ìš© ìˆ˜í–‰
                result = await self._perform_spa_interactions(page, url)

                return result

            except Exception as e:
                return SPAScrapingResult(
                    url=url,
                    products=[],
                    interactions_performed=0,
                    content_states=[],
                    error=str(e),
                    processing_time=time.time() - start_time
                )
            finally:
                await browser.close()

    async def _perform_spa_interactions(self, page: Page, url: str) -> SPAScrapingResult:
        """SPA í˜ì´ì§€ì—ì„œ ë‹¤ì–‘í•œ ìƒí˜¸ì‘ìš©ì„ ìˆ˜í–‰í•˜ì—¬ ì½˜í…ì¸  ìˆ˜ì§‘"""
        start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        all_products = []
        content_states = []
        interactions_count = 0
        seen_content_hashes = set()

        # ì´ˆê¸° ì½˜í…ì¸  ìº¡ì²˜
        initial_content = await page.content()
        initial_hash = hash(initial_content[:5000])  # ì²˜ìŒ 5000ìë§Œ í•´ì‹œ
        seen_content_hashes.add(initial_hash)

        try:
            # ì´ˆê¸° ìƒíƒœì—ì„œ ë°ì´í„° ì¶”ì¶œ
            initial_products = await self._extract_products_from_content(initial_content, url)
            all_products.extend(initial_products)
            content_states.append(f"Initial: {len(initial_products)} products")
            print(f"ğŸ“„ ì´ˆê¸° ìƒíƒœ: {len(initial_products)}ê°œ ìƒí’ˆ ë°œê²¬")

        except Exception as e:
            print(f"âš ï¸ ì´ˆê¸° ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")

        # ìŠ¤í¬ë¡¤ ìˆ˜í–‰ (ì¶”ê°€ ì½˜í…ì¸  ë¡œë”©ì„ ìœ„í•´)
        if self.spa_config.scroll_behavior:
            await self._perform_scroll_interactions(page)

        # í´ë¦­ ê°€ëŠ¥í•œ ìš”ì†Œë“¤ê³¼ ìƒí˜¸ì‘ìš©
        for click_selector in self.spa_config.click_elements:
            if interactions_count >= self.spa_config.max_interactions:
                break

            try:
                # ìš”ì†Œë“¤ ì°¾ê¸°
                elements = await page.query_selector_all(click_selector)

                if not elements:
                    continue

                print(f"ğŸ–±ï¸  '{click_selector}' ìš”ì†Œ {len(elements)}ê°œ ë°œê²¬")

                # ê° ìš”ì†Œì™€ ìƒí˜¸ì‘ìš©
                for i, element in enumerate(elements[:5]):  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
                    if interactions_count >= self.spa_config.max_interactions:
                        break

                    try:
                        # ìš”ì†Œê°€ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ê³  í´ë¦­
                        if await element.is_visible():
                            await element.scroll_into_view_if_needed()
                            await asyncio.sleep(1)

                            # í´ë¦­ ìˆ˜í–‰
                            await element.click()
                            interactions_count += 1

                            # í´ë¦­ í›„ ëŒ€ê¸°
                            await asyncio.sleep(self.spa_config.wait_time)

                            # ë„¤íŠ¸ì›Œí¬ ì•ˆì •í™” ëŒ€ê¸°
                            try:
                                await page.wait_for_load_state('networkidle', timeout=5000)
                            except PlaywrightTimeoutError:
                                pass

                            # ìƒˆë¡œìš´ ì½˜í…ì¸  í™•ì¸ ë° ì¶”ì¶œ
                            new_content = await page.content()
                            new_hash = hash(new_content[:5000])

                            if new_hash not in seen_content_hashes:
                                seen_content_hashes.add(new_hash)

                                try:
                                    new_products = await self._extract_products_from_content(new_content, url)
                                    unique_new_products = self._filter_unique_products(new_products, all_products)

                                    if unique_new_products:
                                        all_products.extend(unique_new_products)
                                        content_states.append(
                                            f"Click {interactions_count} ({click_selector}[{i}]): {len(unique_new_products)} new products"
                                        )
                                        print(f"âœ… í´ë¦­ {interactions_count}: {len(unique_new_products)}ê°œ ìƒˆ ìƒí’ˆ ë°œê²¬")
                                    else:
                                        print(f"âšª í´ë¦­ {interactions_count}: ìƒˆ ìƒí’ˆ ì—†ìŒ")

                                except Exception as e:
                                    print(f"âš ï¸ ì½˜í…ì¸  ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
                            else:
                                print(f"âšª í´ë¦­ {interactions_count}: ì¤‘ë³µ ì½˜í…ì¸ ")

                    except Exception as e:
                        print(f"âš ï¸ ìš”ì†Œ í´ë¦­ ì˜¤ë¥˜: {str(e)}")
                        continue

            except Exception as e:
                print(f"âš ï¸ ì„ íƒì ì²˜ë¦¬ ì˜¤ë¥˜ {click_selector}: {str(e)}")
                continue

        return SPAScrapingResult(
            url=url,
            products=all_products,
            interactions_performed=interactions_count,
            content_states=content_states,
            processing_time=time.time() - start_time
        )

    async def _perform_scroll_interactions(self, page: Page):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤ì„ í†µí•œ ì¶”ê°€ ì½˜í…ì¸  ë¡œë”©"""
        try:
            # ì—¬ëŸ¬ ë²ˆ ìŠ¤í¬ë¡¤í•˜ì—¬ ì§€ì—° ë¡œë”© ì½˜í…ì¸  í™•ì¸
            for i in range(3):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)

                # ì¤‘ê°„ ìœ„ì¹˜ë¡œ ìŠ¤í¬ë¡¤
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
                await asyncio.sleep(1)

            # ë§¨ ìœ„ë¡œ ëŒì•„ê°€ê¸°
            await page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(1)

        except Exception as e:
            print(f"âš ï¸ ìŠ¤í¬ë¡¤ ì˜¤ë¥˜: {str(e)}")

    async def _extract_products_from_content(self, content: str, url: str) -> List[ProductItem]:
        """HTML ì½˜í…ì¸ ì—ì„œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì½˜í…ì¸ ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
            if len(content) < 1000:
                return []

            # LLMìœ¼ë¡œ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ (ë¹„ë™ê¸° ë©”ì„œë“œ ì‚¬ìš©)
            products = await self.llm_extractor.extract_treatments_from_html_async(content, url)
            return products

        except Exception as e:
            print(f"âš ï¸ LLM ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []

    def _filter_unique_products(
        self,
        new_products: List[ProductItem],
        existing_products: List[ProductItem]
    ) -> List[ProductItem]:
        """ìƒˆë¡œìš´ ìƒí’ˆ ì¤‘ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë“¤ë§Œ í•„í„°ë§"""
        existing_keys = set()
        for product in existing_products:
            key = (product.clinic_name, product.product_name)
            existing_keys.add(key)

        unique_products = []
        for product in new_products:
            key = (product.clinic_name, product.product_name)
            if key not in existing_keys:
                unique_products.append(product)
                existing_keys.add(key)

        return unique_products


class ConfigurableScraper:
    """ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤í¬ë˜í•‘ ë°©ì‹ì„ ì§€ì›í•˜ëŠ” ìŠ¤í¬ë˜í¼"""

    def __init__(self, config: ScrapingConfig, llm_extractor: LLMTreatmentExtractor):
        self.config = config
        self.llm_extractor = llm_extractor

    async def scrape_by_config(self) -> List[ProductItem]:
        """ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ìŠ¤í¬ë˜í•‘ ë°©ì‹ ì„ íƒ"""

        if self.config.source_type == ScrapingSourceType.SPA_DYNAMIC:
            return await self._scrape_spa_sites()

        elif self.config.source_type == ScrapingSourceType.STATIC_URLS:
            return await self._scrape_static_urls()

        elif self.config.source_type == ScrapingSourceType.SITEMAP:
            # ê¸°ì¡´ sitemap ë°©ì‹ í˜¸ì¶œ (async_llm_scraper ì‚¬ìš©)
            from src.scrapers.async_llm_scraper import AsyncLLMTreatmentScraper
            scraper = AsyncLLMTreatmentScraper(
                self.config.site_name,
                self.config.base_url,
                self.llm_extractor.api_key,
                max_pages=20,
                max_concurrent=2
            )
            return await scraper.scrape_all_treatments()

        else:
            # BASE_URL - ì¼ë°˜ì ì¸ í¬ë¡¤ë§
            return await self._scrape_base_url()

    async def _scrape_spa_sites(self) -> List[ProductItem]:
        """SPA ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
        spa_scraper = SPAContentScraper(self.config, self.llm_extractor)
        all_products = []

        # static_urlsì´ ìˆìœ¼ë©´ ê·¸ê²ƒë“¤ì„ ì‚¬ìš©, ì—†ìœ¼ë©´ base_url ì‚¬ìš©
        urls_to_scrape = self.config.static_urls if self.config.static_urls else [self.config.base_url]

        for url in urls_to_scrape:
            print(f"ğŸš€ SPA ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            result = await spa_scraper.scrape_spa_content(url)

            if result.error:
                print(f"âŒ SPA ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ({url}): {result.error}")
            else:
                print(f"âœ… SPA ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({url}): {len(result.products)}ê°œ ìƒí’ˆ, {result.interactions_performed}ë²ˆ ìƒí˜¸ì‘ìš©")
                all_products.extend(result.products)

            # ìš”ì²­ ê°„ ê°„ê²©
            await asyncio.sleep(self.config.rate_limit)

        return all_products

    async def _scrape_static_urls(self) -> List[ProductItem]:
        """ì •ì  URL ëª©ë¡ ìŠ¤í¬ë˜í•‘"""
        all_products = []

        for url in self.config.static_urls:
            try:
                products = await self.llm_extractor.extract_treatments_from_url(url)
                all_products.extend(products)
                print(f"âœ… URL ì²˜ë¦¬ ì™„ë£Œ: {url} ({len(products)}ê°œ ìƒí’ˆ)")

                await asyncio.sleep(self.config.rate_limit)

            except Exception as e:
                print(f"âŒ URL ì²˜ë¦¬ ì˜¤ë¥˜ ({url}): {str(e)}")

        return all_products

    async def _scrape_base_url(self) -> List[ProductItem]:
        """Base URLì—ì„œ ê¸°ë³¸ í¬ë¡¤ë§"""
        try:
            products = await self.llm_extractor.extract_treatments_from_url(self.config.base_url)
            print(f"âœ… Base URL ì²˜ë¦¬ ì™„ë£Œ: {len(products)}ê°œ ìƒí’ˆ")
            return products
        except Exception as e:
            print(f"âŒ Base URL ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return []