import asyncio
from typing import List

from src.models.schemas import ProductItem, ScrapingConfig, ScrapingSourceType
from src.scrapers.sitemap_scraper import SitemapScraper
from src.scrapers.spa_scraper import SPAContentScraper
from src.utils.llm_extractor import LLMTreatmentExtractor


class ConfigurableScraper:
    """í†µí•© ì„¤ì • ê¸°ë°˜ ìŠ¤í¬ë˜í¼ (Claude/Gemini ì§€ì›)"""

    def __init__(self, config: ScrapingConfig, llm_extractor: LLMTreatmentExtractor):
        self.config = config
        self.llm_extractor = llm_extractor

    async def scrape_by_config(self) -> List[ProductItem]:
        """ì„¤ì •ì— ë”°ë¼ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰"""
        all_products = []

        if self.config.source_type == ScrapingSourceType.STATIC_URLS:
            # ì •ì  URL ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ (Promise.all ë°©ì‹)
            print(f"ğŸš€ {len(self.config.static_urls)}ê°œ URL ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

            async def scrape_single_url(url: str) -> List[ProductItem]:
                """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘"""
                try:
                    print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
                    products = await self.llm_extractor.extract_treatments_from_url(url)
                    print(f"âœ… {url}: {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ")
                    return products
                except Exception as e:
                    print(f"âŒ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {url}: {str(e)}")
                    return []

            # ëª¨ë“  URLì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (Promise.allê³¼ ë™ì¼)
            tasks = [scrape_single_url(url) for url in self.config.static_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(
                        f"âŒ URL {self.config.static_urls[i]} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {str(result)}"
                    )
                else:
                    all_products.extend(result)

            print(f"ğŸ‰ ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(all_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")

        elif self.config.source_type == ScrapingSourceType.SPA_DYNAMIC:
            # SPA ë™ì  ìŠ¤í¬ë˜í•‘
            if not self.config.spa_config:
                raise ValueError("SPA ìŠ¤í¬ë˜í•‘ì—ëŠ” spa_configê°€ í•„ìš”í•©ë‹ˆë‹¤")

            spa_scraper = SPAContentScraper(self.config, self.llm_extractor)

            # ì‹œì‘ URL ê²°ì •
            start_url = (
                self.config.static_urls[0]
                if self.config.static_urls
                else self.config.base_url
            )

            result = await spa_scraper.scrape_spa_content(start_url)

            if result.error:
                print(f"âŒ SPA ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {result.error}")
            else:
                all_products.extend(result.products)
                print(
                    f"âœ… SPA ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(result.products)}ê°œ ì œí’ˆ, {result.interactions_performed}ë²ˆ ìƒí˜¸ì‘ìš©"
                )

        elif self.config.source_type == ScrapingSourceType.SITEMAP:
            # Sitemap ê¸°ë°˜ ìŠ¤í¬ë˜í•‘
            sitemap_scraper = SitemapScraper(self.config, self.llm_extractor)

            result_products = await sitemap_scraper.scrape_sitemap_content()
            all_products.extend(result_products)

            print(f"âœ… Sitemap ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(result_products)}ê°œ ì œí’ˆ")

        return all_products
