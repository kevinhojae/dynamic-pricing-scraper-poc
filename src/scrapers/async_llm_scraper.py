import asyncio
import time
from typing import List, Set, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from dataclasses import dataclass

try:
    import aiohttp
except ImportError:
    print(
        "aiohttpê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install aiohttp ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    )
    raise

try:
    from tqdm.asyncio import tqdm
except ImportError:
    from tqdm import tqdm

from src.models.schemas import ProductItem, ScrapingConfig
from src.utils.llm_extractor import LLMTreatmentExtractor


@dataclass
class LLMCrawlResult:
    url: str
    content: Optional[str]
    status_code: int
    products: List[ProductItem]
    error: Optional[str] = None
    processing_time: float = 0.0
    llm_processing_time: float = 0.0


class AsyncLLMWebCrawler:
    def __init__(
        self,
        config: ScrapingConfig,
        llm_extractor: LLMTreatmentExtractor,
        max_pages: int = 50,
        max_concurrent: int = 3,
    ):
        self.config = config
        self.llm_extractor = llm_extractor
        self.max_pages = max_pages
        self.max_concurrent = max_concurrent  # LLM ë•Œë¬¸ì— ë” ë‚®ê²Œ ì„¤ì •
        self.visited_urls: Set[str] = set()
        self.found_urls: Set[str] = set()
        self.session: Optional[aiohttp.ClientSession] = None
        self.crawl_start_time = time.time()
        self.max_crawl_time = 300  # 5ë¶„ ìµœëŒ€ í¬ë¡¤ë§ ì‹œê°„ìœ¼ë¡œ ì¦ê°€

    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=self.max_concurrent, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(
            total=30, connect=10
        )  # íƒ€ì„ì•„ì›ƒ ì¤„ì—¬ì„œ ë¹ ë¥¸ ì‹¤íŒ¨
        self.session = aiohttp.ClientSession(
            connector=connector, timeout=timeout, headers=self.config.headers
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def _should_stop_crawling(self) -> bool:
        """í¬ë¡¤ë§ ì¤‘ë‹¨ ì¡°ê±´ ì²´í¬"""
        elapsed_time = time.time() - self.crawl_start_time

        # ì‹œê°„ ì œí•œ ì´ˆê³¼
        if elapsed_time > self.max_crawl_time:
            print(f"â° ìµœëŒ€ í¬ë¡¤ë§ ì‹œê°„({self.max_crawl_time}ì´ˆ) ì´ˆê³¼ë¡œ ì¤‘ë‹¨")
            return True

        # í˜ì´ì§€ ìˆ˜ ì œí•œ ì´ˆê³¼
        if len(self.visited_urls) >= self.max_pages:
            print(f"ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜({self.max_pages}) ë„ë‹¬ë¡œ ì¤‘ë‹¨")
            return True

        return False

    def _is_valid_url(self, url: str, base_domain: str) -> bool:
        """URLì´ í¬ë¡¤ë§ ëŒ€ìƒì¸ì§€ í™•ì¸ (ì‹œìˆ  ê´€ë ¨ í˜ì´ì§€ ìš°ì„ )"""
        try:
            parsed = urlparse(url)
            base_parsed = urlparse(base_domain)

            # ê°™ì€ ë„ë©”ì¸ë§Œ í¬ë¡¤ë§
            if parsed.netloc != base_parsed.netloc:
                return False

            # ì œì™¸í•  íŒŒì¼ í™•ì¥ìë“¤
            excluded_extensions = {
                ".pdf",
                ".jpg",
                ".jpeg",
                ".png",
                ".gif",
                ".css",
                ".js",
                ".ico",
                ".zip",
                ".doc",
                ".docx",
                ".xml",
                ".json",
                ".txt",
                ".csv",
            }
            if any(url.lower().endswith(ext) for ext in excluded_extensions):
                return False

            # ì œì™¸í•  URL íŒ¨í„´ë“¤ (ë” ê´€ëŒ€í•˜ê²Œ ìˆ˜ì •)
            excluded_patterns = {
                "javascript:",
                "mailto:",
                "tel:",
                "/admin",
                "/login",
                "/logout",
                "/api/",
                "/ajax/",
            }
            # query parameterëŠ” í—ˆìš© (ì‹œìˆ  ê´€ë ¨ì¼ ìˆ˜ ìˆìŒ)
            url_without_query = url.split("?")[0].lower()
            if any(pattern in url_without_query for pattern in excluded_patterns):
                return False

            # ì´ë¯¸ ë°©ë¬¸í•œ URL ì œì™¸
            if url in self.visited_urls:
                return False

            # URL ê¸¸ì´ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            if len(url) > 300:  # ë” ê¸¸ê²Œ í—ˆìš©
                return False

            return True
        except Exception:
            return False

    def _get_url_priority(self, url: str) -> int:
        """URL ìš°ì„ ìˆœìœ„ ê³„ì‚° (ì‹œìˆ  ê´€ë ¨ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        url_lower = url.lower()

        # ë†’ì€ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ (ì‹œìˆ  ê´€ë ¨)
        high_priority_keywords = [
            "treatment",
            "procedure",
            "service",
            "menu",
            "price",
            "cost",
            "ì‹œìˆ ",
            "ì¹˜ë£Œ",
            "ì„œë¹„ìŠ¤",
            "ë©”ë‰´",
            "ê°€ê²©",
            "ìš”ê¸ˆ",
            "í”„ë¡œê·¸ë¨",
            "dermatology",
            "skin",
            "beauty",
            "clinic",
            "medical",
            "í”¼ë¶€",
            "ë¯¸ìš©",
            "ì„±í˜•",
            "í´ë¦¬ë‹‰",
            "ì˜ë£Œ",
            "ì§„ë£Œ",
        ]

        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ
        medium_priority_keywords = [
            "about",
            "info",
            "detail",
            "center",
            "department",
            "ì†Œê°œ",
            "ì •ë³´",
            "ì„¼í„°",
            "ë¶€ì„œ",
            "ê³¼",
        ]

        priority = 0

        # ë†’ì€ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ì²´í¬
        for keyword in high_priority_keywords:
            if keyword in url_lower:
                priority += 10

        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ì²´í¬
        for keyword in medium_priority_keywords:
            if keyword in url_lower:
                priority += 5

        # ë©”ì¸ í˜ì´ì§€ëŠ” ë‚®ì€ ìš°ì„ ìˆœìœ„
        if url_lower.endswith("/") or url_lower.endswith("/index.html"):
            priority -= 5

        return priority

    async def _get_sitemap_urls(self, base_url: str) -> List[str]:
        """sitemap.xmlì—ì„œ URLë“¤ì„ ì¶”ì¶œ"""
        sitemap_urls = []
        potential_sitemaps = [
            "/sitemap.xml",
            "/sitemap_index.xml",
            "/sitemaps.xml",
            "/sitemap/sitemap.xml",
            "/wp-sitemap.xml",
            "/sitemap-index.xml",
        ]

        for sitemap_path in potential_sitemaps:
            sitemap_url = urljoin(base_url, sitemap_path)
            try:
                async with self.session.get(sitemap_url) as response:
                    if response.status == 200:
                        content = await response.text()
                        parsed_urls = await self._parse_sitemap_content(
                            content, base_url
                        )
                        sitemap_urls.extend(parsed_urls)
                        tqdm.write(
                            f"ğŸ“„ Sitemap ë°œê²¬: {sitemap_url} ({len(parsed_urls)}ê°œ URL)"
                        )
                        break  # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ sitemapë§Œ ì‚¬ìš©
            except Exception:
                continue  # ë‹¤ìŒ sitemap ê²½ë¡œ ì‹œë„

        return sitemap_urls[:100]  # ìµœëŒ€ 100ê°œ URLë¡œ ì œí•œ

    async def _parse_sitemap_content(self, content: str, base_url: str) -> List[str]:
        """sitemap XML ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•˜ì—¬ URL ì¶”ì¶œ"""
        urls = []
        try:
            soup = BeautifulSoup(content, "xml")

            # sitemap indexì¸ ê²½ìš° (ë‹¤ë¥¸ sitemapë“¤ì„ ì°¸ì¡°)
            sitemap_tags = soup.find_all("sitemap")
            if sitemap_tags:
                for sitemap_tag in sitemap_tags:
                    loc_tag = sitemap_tag.find("loc")
                    if loc_tag and loc_tag.text:
                        # ê°œë³„ sitemapì„ ì¶”ê°€ë¡œ íŒŒì‹±
                        try:
                            async with self.session.get(loc_tag.text) as response:
                                if response.status == 200:
                                    sub_content = await response.text()
                                    sub_urls = await self._parse_sitemap_content(
                                        sub_content, base_url
                                    )
                                    urls.extend(sub_urls)
                        except Exception:
                            continue

            # ì¼ë°˜ sitemapì¸ ê²½ìš° (URLë“¤ì„ ì§ì ‘ í¬í•¨)
            url_tags = soup.find_all("url")
            for url_tag in url_tags:
                loc_tag = url_tag.find("loc")
                if loc_tag and loc_tag.text:
                    url = loc_tag.text.strip()

                    # URL í•„í„°ë§ ë° ìš°ì„ ìˆœìœ„ íŒë‹¨
                    if self._is_sitemap_url_relevant(url):
                        urls.append(url)

            # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ URLë“¤ì„ ì•ìœ¼ë¡œ ì •ë ¬
            urls.sort(key=lambda url: self._get_sitemap_url_priority(url), reverse=True)

        except Exception as e:
            tqdm.write(f"âš ï¸ Sitemap íŒŒì‹± ì˜¤ë¥˜: {str(e)}")

        return urls

    def _is_sitemap_url_relevant(self, url: str) -> bool:
        """sitemap URLì´ ì‹œìˆ  ê´€ë ¨ í˜ì´ì§€ì¸ì§€ íŒë‹¨"""
        url_lower = url.lower()

        # ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ì˜ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ íŒ¨í„´ ìš°ì„  ì²´í¬
        import re

        if re.match(r".*xenia\.clinic/ko/products/[a-f0-9-]{36}.*", url_lower):
            return True

        # ì œì™¸í•  URL íŒ¨í„´ë“¤
        excluded_patterns = [
            "/blog/",
            "/news/",
            "/notice/",
            "/event/",
            "/category/",
            "/tag/",
            "/author/",
            "/feed/",
            "/rss/",
            "/atom/",
            ".pdf",
            ".jpg",
            ".png",
            ".gif",
            ".css",
            ".js",
            "/admin/",
            "/login/",
            "/api/",
        ]

        for pattern in excluded_patterns:
            if pattern in url_lower:
                return False

        # ì‹œìˆ  ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í¬í•¨
        relevant_keywords = [
            "treatment",
            "service",
            "procedure",
            "menu",
            "price",
            "cost",
            "reservation",
            "booking",
            "consultation",
            "products",
            "ì‹œìˆ ",
            "ì¹˜ë£Œ",
            "ì„œë¹„ìŠ¤",
            "ë©”ë‰´",
            "ê°€ê²©",
            "ìš”ê¸ˆ",
            "ì˜ˆì•½",
            "ìƒë‹´",
            "í”„ë¡œê·¸ë¨",
            "ì§„ë£Œ",
        ]

        for keyword in relevant_keywords:
            if keyword in url_lower:
                return True

        # ë©”ì¸ í˜ì´ì§€ë‚˜ ì†Œê°œ í˜ì´ì§€ë„ í¬í•¨ (ì‹œìˆ  ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        main_patterns = [
            "/about",
            "/intro",
            "/center",
            "/clinic",
            "ì†Œê°œ",
            "ì„¼í„°",
            "í´ë¦¬ë‹‰",
            "ë³‘ì›",
        ]

        for pattern in main_patterns:
            if pattern in url_lower:
                return True

        return False  # ê¸°ë³¸ì ìœ¼ë¡œ ì œì™¸ (ê°œë³„ ìƒí’ˆ í˜ì´ì§€ ìš°ì„ )

    def _get_sitemap_url_priority(self, url: str) -> int:
        """sitemap URLì˜ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        url_lower = url.lower()
        priority = 0

        # ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ì˜ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ëŠ” ìµœê³  ìš°ì„ ìˆœìœ„
        import re

        if re.match(r".*xenia\.clinic/ko/products/[a-f0-9-]{36}.*", url_lower):
            priority += 100

        # ë†’ì€ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ
        high_priority = [
            "treatment",
            "service",
            "procedure",
            "menu",
            "price",
            "products",
            "ì‹œìˆ ",
            "ì¹˜ë£Œ",
            "ì„œë¹„ìŠ¤",
            "ë©”ë‰´",
            "ê°€ê²©",
            "ì˜ˆì•½",
        ]

        for keyword in high_priority:
            if keyword in url_lower:
                priority += 20

        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ
        medium_priority = [
            "about",
            "center",
            "clinic",
            "medical",
            "ì†Œê°œ",
            "ì„¼í„°",
            "í´ë¦¬ë‹‰",
            "ì˜ë£Œ",
        ]

        for keyword in medium_priority:
            if keyword in url_lower:
                priority += 10

        # URL ê¹Šì´ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ìš°ì„ ìˆœìœ„
        depth = url.count("/") - 2  # http://domain.com/ ì œì™¸
        priority -= depth * 2

        return priority

    def _extract_urls_from_content(self, html_content: str, base_url: str) -> List[str]:
        """HTML ì½˜í…ì¸ ì—ì„œ ëª¨ë“  ê°€ëŠ¥í•œ URLë“¤ì„ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì†ŒìŠ¤ í™œìš©)"""
        urls_with_priority = []
        try:
            # XML ê²½ê³  ë°©ì§€
            if base_url.endswith(".xml") or "xml" in html_content[:100].lower():
                try:
                    soup = BeautifulSoup(html_content, "xml")
                except Exception:
                    soup = BeautifulSoup(html_content, "html.parser")
            else:
                soup = BeautifulSoup(html_content, "html.parser")

            # 1. ê¸°ë³¸ ë§í¬ (<a href>) ì°¾ê¸°
            for link in soup.find_all("a", href=True):
                href = link["href"]
                full_url = urljoin(base_url, href)

                # Query parameter ë³´ì¡´ (ì‹œìˆ  ê´€ë ¨ì¼ ìˆ˜ ìˆìŒ)
                if self._is_valid_url(full_url.split("?")[0], self.config.base_url):
                    priority = self._get_url_priority(full_url)

                    # ë§í¬ í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„
                    link_text = link.get_text(strip=True).lower()
                    text_keywords = [
                        "treatment",
                        "procedure",
                        "service",
                        "price",
                        "menu",
                        "reservation",
                        "ì‹œìˆ ",
                        "ì¹˜ë£Œ",
                        "ì„œë¹„ìŠ¤",
                        "ê°€ê²©",
                        "ë©”ë‰´",
                        "í”„ë¡œê·¸ë¨",
                        "ì˜ˆì•½",
                        "ìƒë‹´",
                    ]
                    for keyword in text_keywords:
                        if keyword in link_text:
                            priority += 15

                    # Query parameter ìˆìœ¼ë©´ ë†’ì€ ìš°ì„ ìˆœìœ„
                    if "?" in full_url:
                        priority += 10

                    urls_with_priority.append((full_url, priority))

            # 2. JavaScript ë‚´ URL íŒ¨í„´ ì°¾ê¸°
            scripts = soup.find_all("script")
            for script in scripts:
                if script.string:
                    script_content = script.string
                    import re

                    # JavaScript ë‚´ URL íŒ¨í„´ë“¤
                    url_patterns = [
                        r'["\']([^"\']*treatment[^"\']*)["\']',
                        r'["\']([^"\']*service[^"\']*)["\']',
                        r'["\']([^"\']*reservation[^"\']*)["\']',
                        r'["\']([^"\']*category[^"\']*)["\']',
                        r'location\.href\s*=\s*["\']([^"\']+)["\']',
                        r'window\.open\(["\']([^"\']+)["\']',
                    ]

                    for pattern in url_patterns:
                        matches = re.findall(pattern, script_content, re.IGNORECASE)
                        for match in matches:
                            if match.startswith("/") or match.startswith("http"):
                                full_url = urljoin(base_url, match)
                                if self._is_valid_url(
                                    full_url.split("?")[0], self.config.base_url
                                ):
                                    priority = (
                                        self._get_url_priority(full_url) + 20
                                    )  # JSì—ì„œ ë°œê²¬ëœ ê²ƒì€ ë†’ì€ ìš°ì„ ìˆœìœ„
                                    urls_with_priority.append((full_url, priority))

            # 3. Form action ì°¾ê¸°
            for form in soup.find_all("form", action=True):
                action = form["action"]
                if action:
                    full_url = urljoin(base_url, action)
                    if self._is_valid_url(full_url.split("?")[0], self.config.base_url):
                        priority = self._get_url_priority(full_url) + 5
                        urls_with_priority.append((full_url, priority))

            # 4. ë²„íŠ¼ì˜ onclick, data-* ì†ì„± í™•ì¸
            for button in soup.find_all(["button", "div", "span"], attrs=True):
                for attr, value in button.attrs.items():
                    if attr in [
                        "onclick",
                        "data-url",
                        "data-href",
                        "data-link",
                    ] and isinstance(value, str):
                        # onclick="location.href='...' íŒ¨í„´ ì¶”ì¶œ
                        import re

                        url_match = re.search(
                            r'["\']([^"\']*(?:treatment|service|reservation)[^"\']*)["\']',
                            value,
                            re.IGNORECASE,
                        )
                        if url_match:
                            potential_url = url_match.group(1)
                            full_url = urljoin(base_url, potential_url)
                            if self._is_valid_url(
                                full_url.split("?")[0], self.config.base_url
                            ):
                                priority = (
                                    self._get_url_priority(full_url) + 25
                                )  # ë²„íŠ¼ì—ì„œ ë°œê²¬ëœ ê²ƒì€ ë§¤ìš° ë†’ì€ ìš°ì„ ìˆœìœ„
                                urls_with_priority.append((full_url, priority))

            # 5. íŠ¹ë³„ ì²˜ë¦¬: GU Clinic ê°™ì€ SPA ì‚¬ì´íŠ¸
            if "gu.clinic" in base_url.lower():
                # GU Clinicì˜ ì•Œë ¤ì§„ ì‹œìˆ  í˜ì´ì§€ë“¤ ì§ì ‘ ì¶”ê°€
                gu_treatment_urls = [
                    "https://gu.clinic/kr/treatment-reservation",
                    "https://gu.clinic/kr/treatment-reservation?categoryId=64094b472967084b5da2837c",
                    "https://gu.clinic/kr/treatment-reservation?categoryId=64094b652967084b5da2837d",
                    "https://gu.clinic/kr/treatment-reservation?categoryId=64094b7a2967084b5da2837e",
                ]
                for url in gu_treatment_urls:
                    urls_with_priority.append((url, 50))  # ë§¤ìš° ë†’ì€ ìš°ì„ ìˆœìœ„

            # 6. sitemap.xml ì²´í¬ (í•œ ë²ˆë§Œ) - ë™ê¸° ë©”ì„œë“œì—ì„œëŠ” ì œì™¸
            # sitemap ì²˜ë¦¬ëŠ” ë³„ë„ ë¹„ë™ê¸° ë©”ì„œë“œì—ì„œ ì²˜ë¦¬

            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_urls = {}
            for url, priority in urls_with_priority:
                if url in unique_urls:
                    unique_urls[url] = max(
                        unique_urls[url], priority
                    )  # ë” ë†’ì€ ìš°ì„ ìˆœìœ„ ìœ ì§€
                else:
                    unique_urls[url] = priority

            # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
            sorted_urls = sorted(unique_urls.items(), key=lambda x: x[1], reverse=True)
            return [url for url, _ in sorted_urls[:40]]  # ë” ë§ì€ URL í—ˆìš©

        except Exception as e:
            print(f"Error extracting URLs from {base_url}: {str(e)}")

        return []

    async def _fetch_and_process_page(
        self, url: str, semaphore: asyncio.Semaphore
    ) -> LLMCrawlResult:
        """ë‹¨ì¼ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ LLMìœ¼ë¡œ ì²˜ë¦¬"""
        async with semaphore:
            start_time = time.time()
            try:
                await asyncio.sleep(self.config.rate_limit)  # Rate limiting

                # Playwrightë¡œ JavaScript ë Œë”ë§ í›„ LLM ì¶”ì¶œ (ë” ì´ìƒ aiohttp ì‚¬ìš© ì•ˆí•¨)
                llm_start_time = time.time()
                products = await self.llm_extractor.extract_treatments_from_url(url)
                llm_time = time.time() - llm_start_time

                total_time = time.time() - start_time

                return LLMCrawlResult(
                    url=url,
                    content=None,  # Playwrightì—ì„œëŠ” HTML contentë¥¼ ì§ì ‘ ì €ì¥í•˜ì§€ ì•ŠìŒ
                    status_code=200,  # Playwrightë¡œ ì„±ê³µì ìœ¼ë¡œ ë Œë”ë§ë¨
                    products=products,
                    processing_time=total_time,
                    llm_processing_time=llm_time,
                )

            except asyncio.TimeoutError:
                processing_time = time.time() - start_time
                return LLMCrawlResult(
                    url=url,
                    content=None,
                    status_code=0,
                    products=[],
                    error="Timeout",
                    processing_time=processing_time,
                )
            except Exception as e:
                processing_time = time.time() - start_time
                return LLMCrawlResult(
                    url=url,
                    content=None,
                    status_code=0,
                    products=[],
                    error=str(e),
                    processing_time=processing_time,
                )

    async def crawl_and_extract(
        self, start_urls: List[str] = None
    ) -> List[LLMCrawlResult]:
        """ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ë©´ì„œ ë™ì‹œì— LLMìœ¼ë¡œ ì‹œìˆ  ì •ë³´ ì¶”ì¶œ"""
        if not start_urls:
            start_urls = [self.config.base_url]

        urls_to_visit = set(start_urls)

        # sitemapì—ì„œ ì¶”ê°€ URL ìˆ˜ì§‘
        try:
            sitemap_urls = await self._get_sitemap_urls(self.config.base_url)
            if sitemap_urls:
                # ì‹œìˆ  ê´€ë ¨ë„ê°€ ë†’ì€ URLë“¤ì„ ìš°ì„  ì¶”ê°€
                high_priority_sitemap_urls = [
                    url
                    for url in sitemap_urls
                    if self._get_sitemap_url_priority(url) > 15
                ]
                urls_to_visit.update(
                    high_priority_sitemap_urls[:50]
                )  # ë” ë§ì€ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ ì¶”ê°€
                tqdm.write(
                    f"ğŸ“„ Sitemapì—ì„œ {len(high_priority_sitemap_urls)}ê°œ ê³ ìš°ì„ ìˆœìœ„ URL ì¶”ê°€"
                )
        except Exception as e:
            tqdm.write(f"âš ï¸ Sitemap ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

        results = []
        batch_num = 0

        # ë™ì‹œ ì‹¤í–‰ ì œí•œì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (LLM ë•Œë¬¸ì— ë‚®ê²Œ ì„¤ì •)
        semaphore = asyncio.Semaphore(self.max_concurrent)

        # ì§„í–‰ìƒí™© í‘œì‹œ
        pbar = tqdm(desc=f"ğŸ¤– LLM Crawling {self.config.site_name}", unit="pages")

        while urls_to_visit and not self._should_stop_crawling():
            batch_num += 1

            # í˜„ì¬ ë°°ì¹˜ì—ì„œ ì²˜ë¦¬í•  URLë“¤ (ì ì‘ì  ë°°ì¹˜ í¬ê¸°)
            batch_size = min(self.max_concurrent, 5)  # ì¡°ê¸ˆ ë” í° ë°°ì¹˜ë¡œ ì²˜ë¦¬
            current_batch = list(urls_to_visit)[:batch_size]
            urls_to_visit -= set(current_batch)

            # í˜„ì¬ ë°°ì¹˜ì˜ URLë“¤ì„ ë°©ë¬¸í–ˆë‹¤ê³  í‘œì‹œ
            self.visited_urls.update(current_batch)

            pbar.set_description(
                f"ğŸ¤– LLM Crawling {self.config.site_name} (Batch {batch_num})"
            )

            # ë¹„ë™ê¸°ë¡œ í˜ì´ì§€ë“¤ ê°€ì ¸ì˜¤ê³  ì²˜ë¦¬
            tasks = [
                self._fetch_and_process_page(url, semaphore) for url in current_batch
            ]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            successful_extractions = 0
            total_treatments_in_batch = 0

            for result in batch_results:
                if isinstance(result, LLMCrawlResult):
                    results.append(result)
                    pbar.update(1)

                    if result.products:
                        successful_extractions += 1
                        total_treatments_in_batch += sum(
                            len(product.treatments) for product in result.products
                        )

                    # ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜¨ í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ URLë“¤ ì¶”ì¶œ
                    if result.content and result.status_code == 200:
                        new_urls = self._extract_urls_from_content(
                            result.content, result.url
                        )
                        unvisited_urls = [
                            url for url in new_urls if url not in self.visited_urls
                        ]

                        # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ URLë“¤ì„ ë” ë§ì´ ì¶”ê°€
                        limited_new_urls = unvisited_urls[:15]  # ë” ë§ì€ URL í—ˆìš©
                        urls_to_visit.update(limited_new_urls)

                        # ë””ë²„ê¹…: URLê³¼ ì½˜í…ì¸  ê¸¸ì´ ì¶œë ¥
                        if len(result.content) < 1000:
                            tqdm.write(
                                f"âš ï¸  ì§§ì€ ì½˜í…ì¸ : {result.url} ({len(result.content)} chars)"
                            )
                        else:
                            tqdm.write(
                                f"âœ… ê¸´ ì½˜í…ì¸ : {result.url} ({len(result.content)} chars)"
                            )

                    # ìƒì„¸ ì§„í–‰ ì •ë³´ í‘œì‹œ
                    pbar.set_postfix(
                        {
                            "Found": len(self.visited_urls),
                            "Queue": len(urls_to_visit),
                            "Treatments": len(
                                [
                                    treatment
                                    for r in results
                                    for product in r.products
                                    for treatment in product.treatments
                                ]
                            ),
                            "LLM_Avg": (
                                f"{result.llm_processing_time:.1f}s"
                                if result.llm_processing_time > 0
                                else "0s"
                            ),
                        }
                    )

            # ë°°ì¹˜ ê²°ê³¼ ì¶œë ¥
            if successful_extractions > 0:
                tqdm.write(
                    f"âœ… Batch {batch_num}: {successful_extractions}ê°œ í˜ì´ì§€ì—ì„œ {total_treatments_in_batch}ê°œ ì‹œìˆ  ì¶”ì¶œ"
                )

            # í¬ë¡¤ë§ ì§„í–‰ìƒí™© ì²´í¬
            if len(urls_to_visit) == 0:
                tqdm.write("ğŸ“ ë” ì´ìƒ í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
                break

        pbar.close()
        return results


class AsyncLLMTreatmentScraper:
    def __init__(
        self,
        site_name: str,
        base_url: str,
        api_key: str,
        max_pages: int = 15,
        max_concurrent: int = 2,
    ):  # ë” ë³´ìˆ˜ì ì¸ ê¸°ë³¸ê°’
        self.site_name = site_name
        self.base_url = base_url
        self.api_key = api_key
        self.max_pages = max_pages
        self.max_concurrent = max_concurrent

    async def scrape_all_treatments(self) -> List[ProductItem]:
        """ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ LLMìœ¼ë¡œ ì‹œìˆ  ì •ë³´ë¥¼ ë¹„ë™ê¸° ì¶”ì¶œ"""

        config = ScrapingConfig(
            site_name=self.site_name,
            base_url=self.base_url,
            selectors={},
            use_selenium=False,  # ë¹„ë™ê¸°ì—ì„œëŠ” requests ì‚¬ìš©
            rate_limit=0.8,  # LLM ì†ë„ ê°œì„ ìœ¼ë¡œ ë” ë¹ ë¥´ê²Œ
        )

        llm_extractor = LLMTreatmentExtractor(self.api_key)

        try:
            async with AsyncLLMWebCrawler(
                config,
                llm_extractor,
                max_pages=self.max_pages,
                max_concurrent=self.max_concurrent,
            ) as crawler:
                print(f"ğŸš€ Starting async LLM crawl of {self.site_name}...")
                crawl_results = await crawler.crawl_and_extract()

                successful_pages = [
                    r for r in crawl_results if r.status_code == 200 and r.products
                ]
                print(
                    f"âœ… Successfully processed {len(successful_pages)} pages with products"
                )

                # ëª¨ë“  ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
                all_products = []
                for result in crawl_results:
                    all_products.extend(result.products)

                print(f"ğŸ“¦ Total extracted: {len(all_products)} products")

                # ì¤‘ë³µ ì œê±° (ê°™ì€ ìƒí’ˆëª…ê³¼ í´ë¦¬ë‹‰ëª…)
                unique_products = []
                seen = set()
                for product in all_products:
                    key = (
                        product.clinic_name,
                        product.product_name,
                    )
                    if key not in seen:
                        seen.add(key)
                        unique_products.append(product)

                print(f"ğŸ¯ Final count: {len(unique_products)} unique products")

                # ì´ ì‹œìˆ  ê°œìˆ˜ ê³„ì‚°
                total_treatments = sum(
                    len(product.treatments) for product in unique_products
                )
                print(
                    f"ğŸ’‰ Total treatments: {total_treatments} treatments across all products"
                )

                # í†µê³„ ì¶œë ¥
                total_llm_time = sum(
                    r.llm_processing_time
                    for r in crawl_results
                    if r.llm_processing_time
                )
                avg_llm_time = (
                    total_llm_time
                    / len([r for r in crawl_results if r.llm_processing_time])
                    if crawl_results
                    else 0
                )
                print(f"ğŸ“Š Average LLM processing time: {avg_llm_time:.2f}s per page")

                return unique_products

        except Exception as e:
            print(f"âŒ Error in async LLM scraping: {str(e)}")
            return []


# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
async def run_async_llm_scraping_demo(api_key: str):
    """ë¹„ë™ê¸° LLM ìŠ¤í¬ë˜í•‘ ë°ëª¨ ì‹¤í–‰"""

    scrapers_config = [
        ("GU Clinic", "https://gu.clinic/", 20, 2),  # ë” ë§ì€ í˜ì´ì§€ì™€ ë™ì‹œì„±
        ("Beauty Leader", "https://beautyleader.co.kr/", 15, 2),
        ("Feeline Network", "http://sinchon.feeline.network/skinclinic_", 15, 2),
    ]

    all_treatments = []
    scraping_results = {}

    # íƒ€ì„ì•„ì›ƒ ë“±ìœ¼ë¡œ ì¤‘ë‹¨ë˜ì–´ë„ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ í•¨ìˆ˜
    def save_partial_results():
        if all_treatments:
            from datetime import datetime
            import os
            import json

            os.makedirs("data/raw", exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ë¶€ë¶„ ê²°ê³¼ ì €ì¥
            with open(
                f"data/raw/partial_treatments_{timestamp}.json", "w", encoding="utf-8"
            ) as f:
                json.dump(
                    [t.model_dump() for t in all_treatments],
                    f,
                    ensure_ascii=False,
                    indent=2,
                    default=str,
                )

            with open(
                f"data/raw/partial_scraping_results_{timestamp}.json",
                "w",
                encoding="utf-8",
            ) as f:
                json.dump(
                    scraping_results, f, ensure_ascii=False, indent=2, default=str
                )

            print("\nğŸ’¾ ë¶€ë¶„ ê²°ê³¼ ì €ì¥ë¨:")
            print(f"   - ì‹œìˆ  ì •ë³´: {len(all_treatments)}ê°œ")
            print(f"   - íŒŒì¼: data/raw/partial_treatments_{timestamp}.json")

    try:
        for name, base_url, max_pages, concurrent in scrapers_config:
            start_time = time.time()
            print(f"\nğŸš€ Starting async LLM scraping for {name}...")

            try:
                async_llm_scraper = AsyncLLMTreatmentScraper(
                    name,
                    base_url,
                    api_key,
                    max_pages=max_pages,
                    max_concurrent=concurrent,
                )

                treatments = await async_llm_scraper.scrape_all_treatments()
                all_treatments.extend(treatments)

                duration = time.time() - start_time
                scraping_results[name] = {
                    "success": True,
                    "treatments_found": len(treatments),
                    "error": None,
                    "duration_seconds": round(duration, 2),
                    "pages_crawled": max_pages,
                    "concurrent_workers": concurrent,
                    "scraping_method": "async_llm",
                }

                print(f"âœ… {name}: {len(treatments)} treatments in {duration:.1f}s")

            except Exception as e:
                duration = time.time() - start_time
                scraping_results[name] = {
                    "success": False,
                    "treatments_found": 0,
                    "error": str(e),
                    "duration_seconds": round(duration, 2),
                    "scraping_method": "async_llm",
                }
                print(f"âŒ {name}: Error - {str(e)}")

    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        save_partial_results()
    except Exception as e:
        print(f"\nâŒ ì „ì²´ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")
        save_partial_results()

    print(f"\nğŸ‰ Total treatments collected: {len(all_treatments)}")
    return all_treatments, scraping_results


def run_async_llm_scraping(api_key: str):
    """ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° LLM ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
    return asyncio.run(run_async_llm_scraping_demo(api_key))
