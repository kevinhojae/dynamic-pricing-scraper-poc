"""
Sitemap ê¸°ë°˜ ìŠ¤í¬ë˜í•‘ì„ ë‹´ë‹¹í•˜ëŠ” ì „ìš© ìŠ¤í¬ë˜í¼
Claude/Gemini ì§€ì›
"""

import asyncio
import aiohttp
from typing import List
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from tqdm import tqdm

from src.models.schemas import ProductItem, ScrapingConfig
from src.utils.llm_extractor import LLMTreatmentExtractor


class SitemapScraper:
    """Sitemap ê¸°ë°˜ URL ìˆ˜ì§‘ ë° ìŠ¤í¬ë˜í•‘ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ (Claude/Gemini ì§€ì›)"""

    def __init__(self, config: ScrapingConfig, llm_extractor: LLMTreatmentExtractor):
        self.config = config
        self.llm_extractor = llm_extractor

    async def scrape_sitemap_content(self) -> List[ProductItem]:
        """Sitemap ê¸°ë°˜ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰"""
        print(f"ğŸ—ºï¸  Sitemap ìŠ¤í¬ë˜í•‘ ì‹œì‘: {self.config.base_url}")

        # HTTP ì„¸ì…˜ ìƒì„±
        async with aiohttp.ClientSession() as session:
            # Sitemapì—ì„œ URLë“¤ ìˆ˜ì§‘
            urls = await self._get_sitemap_urls(session, self.config.base_url)

            if not urls:
                print("âš ï¸  Sitemapì—ì„œ ìœ íš¨í•œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            print(f"ğŸ“„ Sitemapì—ì„œ {len(urls)}ê°œ URL ë°œê²¬")

            # URLë“¤ì„ ë³‘ë ¬ë¡œ ìŠ¤í¬ë˜í•‘
            all_products = []

            async def scrape_single_url(url: str) -> List[ProductItem]:
                """ë‹¨ì¼ URL ìŠ¤í¬ë˜í•‘"""
                try:
                    tqdm.write(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
                    products = await self.llm_extractor.extract_treatments_from_url(url)
                    tqdm.write(f"âœ… {url}: {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ")
                    return products
                except Exception as e:
                    tqdm.write(f"âŒ URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {url}: {str(e)}")
                    return []

            # ìµœëŒ€ 50ê°œ URLë¡œ ì œí•œí•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
            limited_urls = urls[:50]
            print(f"ğŸš€ {len(limited_urls)}ê°œ URL ë³‘ë ¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

            tasks = [scrape_single_url(url) for url in limited_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    tqdm.write(f"âŒ URL {limited_urls[i]} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {str(result)}")
                else:
                    all_products.extend(result)

            print(f"ğŸ‰ Sitemap ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(all_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
            return all_products

    async def _get_sitemap_urls(
        self, session: aiohttp.ClientSession, base_url: str
    ) -> List[str]:
        """sitemap.xmlì—ì„œ URLë“¤ì„ ì¶”ì¶œ"""
        sitemap_urls = []
        potential_sitemaps = [
            "/sitemap.xml",
            "/sitemap_index.xml",
            "/sitemaps.xml",
            "/sitemap/sitemap.xml",
            "/wp-sitemap.xml",
            "/sitemap-index.xml",
        ]

        for sitemap_path in potential_sitemaps:
            sitemap_url = urljoin(base_url, sitemap_path)
            try:
                async with session.get(sitemap_url) as response:
                    if response.status == 200:
                        content = await response.text()
                        parsed_urls = await self._parse_sitemap_content(
                            session, content, base_url
                        )
                        sitemap_urls.extend(parsed_urls)
                        tqdm.write(
                            f"ğŸ“„ Sitemap ë°œê²¬: {sitemap_url} ({len(parsed_urls)}ê°œ URL)"
                        )
                        break  # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ sitemapë§Œ ì‚¬ìš©
            except Exception as e:
                tqdm.write(f"âš ï¸  Sitemap ì ‘ê·¼ ì‹¤íŒ¨ {sitemap_url}: {str(e)}")
                continue  # ë‹¤ìŒ sitemap ê²½ë¡œ ì‹œë„

        return sitemap_urls[:100]  # ìµœëŒ€ 100ê°œ URLë¡œ ì œí•œ

    async def _parse_sitemap_content(
        self, session: aiohttp.ClientSession, content: str, base_url: str
    ) -> List[str]:
        """sitemap XML ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•˜ì—¬ URL ì¶”ì¶œ"""
        urls = []
        try:
            soup = BeautifulSoup(content, "xml")

            # sitemap indexì¸ ê²½ìš° (ë‹¤ë¥¸ sitemapë“¤ì„ ì°¸ì¡°)
            sitemap_tags = soup.find_all("sitemap")
            if sitemap_tags:
                for sitemap_tag in sitemap_tags:
                    loc_tag = sitemap_tag.find("loc")
                    if loc_tag and loc_tag.text:
                        # ê°œë³„ sitemapì„ ì¶”ê°€ë¡œ íŒŒì‹±
                        try:
                            async with session.get(loc_tag.text) as response:
                                if response.status == 200:
                                    sub_content = await response.text()
                                    sub_urls = await self._parse_sitemap_content(
                                        session, sub_content, base_url
                                    )
                                    urls.extend(sub_urls)
                        except Exception:
                            continue

            # ì¼ë°˜ sitemapì¸ ê²½ìš° (URLë“¤ì„ ì§ì ‘ í¬í•¨)
            url_tags = soup.find_all("url")
            for url_tag in url_tags:
                loc_tag = url_tag.find("loc")
                if loc_tag and loc_tag.text:
                    url = loc_tag.text.strip()

                    # URL í•„í„°ë§ ë° ìš°ì„ ìˆœìœ„ íŒë‹¨
                    if self._is_sitemap_url_relevant(url):
                        urls.append(url)

            # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ URLë“¤ì„ ì•ìœ¼ë¡œ ì •ë ¬
            urls.sort(key=lambda url: self._get_sitemap_url_priority(url), reverse=True)

        except Exception as e:
            tqdm.write(f"âš ï¸ Sitemap íŒŒì‹± ì˜¤ë¥˜: {str(e)}")

        return urls

    def _is_sitemap_url_relevant(self, url: str) -> bool:
        """sitemap URLì´ ì‹œìˆ  ê´€ë ¨ í˜ì´ì§€ì¸ì§€ íŒë‹¨"""
        url_lower = url.lower()

        # ì„¤ì •ì—ì„œ exclude_patterns í™•ì¸
        if hasattr(self.config, "custom_settings") and self.config.custom_settings:
            exclude_patterns = self.config.custom_settings.get("exclude_patterns", [])
            for pattern in exclude_patterns:
                if pattern in url_lower:
                    return False

        # ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ì˜ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ íŒ¨í„´ ìš°ì„  ì²´í¬
        import re

        if re.match(r".*xenia\.clinic/ko/products/[a-f0-9-]{36}.*", url_lower):
            return True

        # ì œì™¸í•  URL íŒ¨í„´ë“¤
        excluded_patterns = [
            "/blog/",
            "/news/",
            "/notice/",
            "/event/",
            "/category/",
            "/tag/",
            "/author/",
            "/feed/",
            "/rss/",
            "/atom/",
            ".pdf",
            ".jpg",
            ".png",
            ".gif",
            ".css",
            ".js",
            "/admin/",
            "/login/",
            "/api/",
        ]

        for pattern in excluded_patterns:
            if pattern in url_lower:
                return False

        # ì„¤ì •ì—ì„œ priority_keywords í™•ì¸
        if hasattr(self.config, "custom_settings") and self.config.custom_settings:
            priority_keywords = self.config.custom_settings.get("priority_keywords", [])
            for keyword in priority_keywords:
                if keyword in url_lower:
                    return True

        # ì‹œìˆ  ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í¬í•¨
        relevant_keywords = [
            "treatment",
            "service",
            "procedure",
            "menu",
            "price",
            "cost",
            "reservation",
            "booking",
            "consultation",
            "products",
            "ì‹œìˆ ",
            "ì¹˜ë£Œ",
            "ì„œë¹„ìŠ¤",
            "ë©”ë‰´",
            "ê°€ê²©",
            "ìš”ê¸ˆ",
            "ì˜ˆì•½",
            "ìƒë‹´",
            "í”„ë¡œê·¸ë¨",
            "ì§„ë£Œ",
        ]

        for keyword in relevant_keywords:
            if keyword in url_lower:
                return True

        return False

    def _get_sitemap_url_priority(self, url: str) -> int:
        """sitemap URLì˜ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        url_lower = url.lower()
        priority = 0

        # ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ (UUID íŒ¨í„´)
        import re

        if re.match(r".*xenia\.clinic/ko/products/[a-f0-9-]{36}.*", url_lower):
            priority += 50  # ë§¤ìš° ë†’ì€ ìš°ì„ ìˆœìœ„

        # ì„¤ì •ì˜ priority_keywords í™•ì¸
        if hasattr(self.config, "custom_settings") and self.config.custom_settings:
            priority_keywords = self.config.custom_settings.get("priority_keywords", [])
            for keyword in priority_keywords:
                if keyword in url_lower:
                    priority += 20

        # ì‹œìˆ  ê´€ë ¨ í‚¤ì›Œë“œ
        treatment_keywords = ["treatment", "procedure", "service", "ì‹œìˆ ", "ì¹˜ë£Œ"]
        for keyword in treatment_keywords:
            if keyword in url_lower:
                priority += 15

        # ì œí’ˆ/ë©”ë‰´ ê´€ë ¨ í‚¤ì›Œë“œ
        product_keywords = ["products", "menu", "price", "ì œí’ˆ", "ë©”ë‰´", "ê°€ê²©"]
        for keyword in product_keywords:
            if keyword in url_lower:
                priority += 10

        # ì˜ˆì•½ ê´€ë ¨ í‚¤ì›Œë“œ
        booking_keywords = ["reservation", "booking", "consultation", "ì˜ˆì•½", "ìƒë‹´"]
        for keyword in booking_keywords:
            if keyword in url_lower:
                priority += 8

        # ë©”ì¸ í˜ì´ì§€ëŠ” ë‚®ì€ ìš°ì„ ìˆœìœ„
        if url_lower.endswith("/") or url_lower.endswith("/index.html"):
            priority -= 5

        return priority
