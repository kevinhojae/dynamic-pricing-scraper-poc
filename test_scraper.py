#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ ì „ìš© ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import json
import os
from datetime import datetime
from typing import List

# Environment variables
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ì˜ì¡´ì„±
from src.scrapers.async_llm_scraper import AsyncLLMTreatmentScraper
from src.models.schemas import TreatmentItem

def load_gemini_api_key() -> str:
    """Gemini API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ"""
    # .env íŒŒì¼ ë¡œë“œ
    load_dotenv()
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("1. .env íŒŒì¼ì— GEMINI_API_KEY=your-api-key-here")
        print("2. export GEMINI_API_KEY='your-api-key-here'")
        exit(1)
    return api_key

async def scrape_single_site(site_name: str, base_url: str, gemini_api_key: str, max_pages: int = 15) -> List[TreatmentItem]:
    """ë‹¨ì¼ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    print(f"ğŸš€ {site_name} ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    scraper = AsyncLLMTreatmentScraper(
        site_name=site_name,
        base_url=base_url,
        gemini_api_key=gemini_api_key,
        max_pages=max_pages,
        max_concurrent=2
    )

    treatments = await scraper.scrape_all_treatments()
    print(f"âœ… {site_name}: {len(treatments)}ê°œ ì‹œìˆ  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")

    return treatments

def save_results(treatments: List[TreatmentItem], filename: str = None):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_treatments_{timestamp}.json"

    os.makedirs("data", exist_ok=True)
    filepath = f"data/{filename}"

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(
            [treatment.model_dump() for treatment in treatments],
            f,
            ensure_ascii=False,
            indent=2,
            default=str
        )

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {filepath}")
    print(f"ğŸ“Š ì´ {len(treatments)}ê°œ ì‹œìˆ  ì •ë³´")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    gemini_api_key = load_gemini_api_key()

    # ìŠ¤í¬ë˜í•‘í•  ì‚¬ì´íŠ¸ ì„¤ì • - ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ í…ŒìŠ¤íŠ¸
    sites = [
        ("Xenia Clinic", "https://xenia.clinic/ko/products/", 5),
    ]

    all_treatments = []

    for site_name, base_url, max_pages in sites:
        try:
            treatments = await scrape_single_site(site_name, base_url, gemini_api_key, max_pages)
            all_treatments.extend(treatments)
        except Exception as e:
            print(f"âŒ {site_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")

    if all_treatments:
        save_results(all_treatments)
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())