#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘ ì „ìš© ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import json
import os
from datetime import datetime
from typing import List

# Environment variables
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ì˜ì¡´ì„±
from src.scrapers.async_llm_scraper import AsyncLLMTreatmentScraper
from src.models.schemas import ProductItem


def load_api_key() -> str:
    """API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ"""
    # .env íŒŒì¼ ë¡œë“œ
    load_dotenv()

    api_key = os.getenv("ANTHROPIC_AUTH_TOKEN")
    if not api_key:
        print("âŒ ANTHROPIC_AUTH_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("1. .env íŒŒì¼ì— ANTHROPIC_AUTH_TOKEN=your-api-key-here")
        print("2. export ANTHROPIC_AUTH_TOKEN='your-api-key-here'")
        exit(1)
    return api_key


async def scrape_single_site(
    site_name: str, base_url: str, api_key: str, max_pages: int = 15
) -> List[ProductItem]:
    """ë‹¨ì¼ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    print(f"ğŸš€ {site_name} ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    scraper = AsyncLLMTreatmentScraper(
        site_name=site_name,
        base_url=base_url,
        api_key=api_key,
        max_pages=max_pages,
        max_concurrent=2,
    )

    products = await scraper.scrape_all_treatments()
    print(f"âœ… {site_name}: {len(products)}ê°œ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")

    return products


def save_results(products: List[ProductItem], filename: str = None):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_treatments_{timestamp}.json"

    os.makedirs("data", exist_ok=True)
    filepath = f"data/{filename}"

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(
            [product.model_dump() for product in products],
            f,
            ensure_ascii=False,
            indent=2,
            default=str,
        )

    # ì´ ì‹œìˆ  ê°œìˆ˜ ê³„ì‚°
    total_treatments = sum(len(product.treatments) for product in products)

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {filepath}")
    print(f"ğŸ“¦ ì´ {len(products)}ê°œ ìƒí’ˆ ì •ë³´")
    print(f"ğŸ’‰ ì´ {total_treatments}ê°œ ì‹œìˆ  ì •ë³´")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    api_key = load_api_key()

    # ìŠ¤í¬ë˜í•‘í•  ì‚¬ì´íŠ¸ ì„¤ì • - ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ í…ŒìŠ¤íŠ¸
    sites = [
        (
            "Xenia Clinic",
            "https://xenia.clinic/",
            10,
        ),  # ë” ë§ì€ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§
    ]

    all_products = []

    for site_name, base_url, max_pages in sites:
        try:
            products = await scrape_single_site(site_name, base_url, api_key, max_pages)
            all_products.extend(products)
        except Exception as e:
            print(f"âŒ {site_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")

    if all_products:
        save_results(all_products)
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())
