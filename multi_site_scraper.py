"""
ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í¼ - ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë™ì‹œ ìŠ¤í¬ë˜í•‘
"""
import asyncio
import os
import time
from datetime import datetime
from typing import List, Dict, Any

from src.config.site_configs import site_config_manager
from src.scrapers.spa_scraper import ConfigurableScraper
from src.utils.llm_extractor import LLMTreatmentExtractor
from src.models.schemas import ProductItem


class MultiSiteScraper:
    """ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.llm_extractor = LLMTreatmentExtractor(api_key)

    async def scrape_all_sites(self, site_keys: List[str] = None) -> Dict[str, Any]:
        """ì§€ì •ëœ ì‚¬ì´íŠ¸ë“¤ì„ ëª¨ë‘ ìŠ¤í¬ë˜í•‘"""

        if site_keys is None:
            site_keys = site_config_manager.list_sites()

        print(f"ğŸš€ ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        print(f"ğŸ“‹ ëŒ€ìƒ ì‚¬ì´íŠ¸: {', '.join(site_keys)}")

        results = {}
        all_products = []

        for site_key in site_keys:
            print(f"\n{'='*50}")
            print(f"ğŸ¥ ì‚¬ì´íŠ¸: {site_key}")

            start_time = time.time()
            config = site_config_manager.get_config(site_key)

            if not config:
                print(f"âŒ {site_key} ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                results[site_key] = {
                    "success": False,
                    "error": "Configuration not found",
                    "products": [],
                    "product_count": 0,
                    "duration": 0
                }
                continue

            try:
                scraper = ConfigurableScraper(config, self.llm_extractor)
                products = await scraper.scrape_by_config()

                duration = time.time() - start_time
                treatment_count = sum(len(product.treatments) for product in products)

                results[site_key] = {
                    "success": True,
                    "error": None,
                    "products": products,
                    "product_count": len(products),
                    "treatment_count": treatment_count,
                    "duration": duration,
                    "source_type": config.source_type.value,
                    "site_name": config.site_name
                }

                all_products.extend(products)

                print(f"âœ… {config.site_name} ì™„ë£Œ")
                print(f"   ğŸ“¦ ìƒí’ˆ: {len(products)}ê°œ")
                print(f"   ğŸ’‰ ì‹œìˆ : {treatment_count}ê°œ")
                print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")

            except Exception as e:
                duration = time.time() - start_time
                print(f"âŒ {site_key} ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")

                results[site_key] = {
                    "success": False,
                    "error": str(e),
                    "products": [],
                    "product_count": 0,
                    "treatment_count": 0,
                    "duration": duration,
                    "source_type": config.source_type.value if config else "unknown",
                    "site_name": config.site_name if config else site_key
                }

        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._print_summary(results, all_products)

        return {
            "results": results,
            "all_products": all_products,
            "summary": self._generate_summary(results, all_products)
        }

    async def scrape_ppeum_only(self) -> List[ProductItem]:
        """ì¨ ê¸€ë¡œë²Œ í´ë¦¬ë‹‰ë§Œ ìŠ¤í¬ë˜í•‘"""
        config = site_config_manager.create_ppeum_global_config()
        scraper = ConfigurableScraper(config, self.llm_extractor)
        return await scraper.scrape_by_config()

    async def scrape_spa_sites_only(self) -> Dict[str, Any]:
        """SPA íƒ€ì… ì‚¬ì´íŠ¸ë“¤ë§Œ ìŠ¤í¬ë˜í•‘"""
        spa_sites = site_config_manager.get_spa_sites()
        return await self.scrape_all_sites(spa_sites)

    def _print_summary(self, results: Dict[str, Any], all_products: List[ProductItem]):
        """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")

        successful_sites = [k for k, v in results.items() if v["success"]]
        failed_sites = [k for k, v in results.items() if not v["success"]]

        print(f"âœ… ì„±ê³µí•œ ì‚¬ì´íŠ¸: {len(successful_sites)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸: {len(failed_sites)}ê°œ")

        if successful_sites:
            print(f"\nğŸ† ì„±ê³µí•œ ì‚¬ì´íŠ¸ë“¤:")
            for site in successful_sites:
                result = results[site]
                print(f"   â€¢ {result['site_name']}: {result['product_count']}ê°œ ìƒí’ˆ, {result['treatment_count']}ê°œ ì‹œìˆ ")

        if failed_sites:
            print(f"\nğŸ’¥ ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸ë“¤:")
            for site in failed_sites:
                result = results[site]
                print(f"   â€¢ {result['site_name']}: {result['error']}")

        total_products = len(all_products)
        total_treatments = sum(len(product.treatments) for product in all_products)
        total_duration = sum(result["duration"] for result in results.values())

        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   ğŸ“¦ ì´ ìƒí’ˆ ìˆ˜: {total_products}ê°œ")
        print(f"   ğŸ’‰ ì´ ì‹œìˆ  ìˆ˜: {total_treatments}ê°œ")
        print(f"   â±ï¸  ì´ ì†Œìš”ì‹œê°„: {total_duration:.1f}ì´ˆ")

        if total_products > 0:
            print(f"   ğŸ“Š í‰ê·  ìƒí’ˆë‹¹ ì‹œìˆ  ìˆ˜: {total_treatments/total_products:.1f}ê°œ")

    def _generate_summary(self, results: Dict[str, Any], all_products: List[ProductItem]) -> Dict[str, Any]:
        """ìš”ì•½ ì •ë³´ ìƒì„±"""
        return {
            "total_sites": len(results),
            "successful_sites": len([r for r in results.values() if r["success"]]),
            "failed_sites": len([r for r in results.values() if not r["success"]]),
            "total_products": len(all_products),
            "total_treatments": sum(len(product.treatments) for product in all_products),
            "total_duration": sum(result["duration"] for result in results.values()),
            "scraping_timestamp": datetime.now().isoformat()
        }

    def save_results(self, scraping_results: Dict[str, Any], suffix: str = "") -> str:
        """ì „ì²´ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs("data/raw", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/raw/multi_site_scraping_{timestamp}{suffix}.json"

        import json

        # ProductItem ê°ì²´ë“¤ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_results = {}
        for site_key, result in scraping_results["results"].items():
            serializable_results[site_key] = {
                **result,
                "products": [product.model_dump() for product in result["products"]]
            }

        save_data = {
            "results": serializable_results,
            "summary": scraping_results["summary"],
            "all_products": [product.model_dump() for product in scraping_results["all_products"]]
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)

        print(f"ğŸ’¾ ì „ì²´ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì‚¬ìš© ì˜ˆì‹œ"""
    # Anthropic API í‚¤ í™•ì¸
    api_key = os.getenv("ANTHROPIC_AUTH_TOKEN")
    if not api_key:
        print("âŒ ANTHROPIC_AUTH_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("1. .env íŒŒì¼ì— ANTHROPIC_AUTH_TOKEN=your-api-key-here")
        print("2. export ANTHROPIC_AUTH_TOKEN='your-api-key-here'")
        return

    # ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í¼ ìƒì„±
    multi_scraper = MultiSiteScraper(api_key)

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì˜µì…˜ë“¤
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í¬ë˜í•‘ ì˜µì…˜:")
    print("1. ëª¨ë“  ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘")
    print("2. ì¨ ê¸€ë¡œë²Œë§Œ ìŠ¤í¬ë˜í•‘")
    print("3. SPA ì‚¬ì´íŠ¸ë“¤ë§Œ ìŠ¤í¬ë˜í•‘")
    print("4. íŠ¹ì • ì‚¬ì´íŠ¸ë“¤ë§Œ ìŠ¤í¬ë˜í•‘")

    # ì˜ˆì‹œ: ì¨ ê¸€ë¡œë²Œë§Œ ìŠ¤í¬ë˜í•‘
    print(f"\nğŸ¯ ì¨ ê¸€ë¡œë²Œ í´ë¦¬ë‹‰ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰...")
    products = await multi_scraper.scrape_ppeum_only()

    if products:
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        import json
        os.makedirs("data/raw", exist_ok=True)
        filename = f"data/raw/ppeum_global_only_{timestamp}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(
                [product.model_dump() for product in products],
                f,
                ensure_ascii=False,
                indent=2,
                default=str
            )

        print(f"ğŸ’¾ ì¨ ê¸€ë¡œë²Œ ê²°ê³¼ ì €ì¥: {filename}")

    # ì „ì²´ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì˜ˆì‹œ (ì£¼ì„ ì²˜ë¦¬)
    # print(f"\nğŸŒ ì „ì²´ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘...")
    # results = await multi_scraper.scrape_all_sites()
    # multi_scraper.save_results(results)


if __name__ == "__main__":
    asyncio.run(main())