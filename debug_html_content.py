#!/usr/bin/env python3
"""
ì‹¤ì œ ìŠ¤í¬ë˜í•‘ë˜ëŠ” HTML ì½˜í…ì¸ ë¥¼ í™•ì¸í•˜ëŠ” ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
1. HTML êµ¬ì¡° í™•ì¸
2. í…ìŠ¤íŠ¸ ì¶”ì¶œ ê³¼ì • í™•ì¸
3. ì „ì²˜ë¦¬ í›„ LLMì—ê²Œ ì „ë‹¬ë˜ëŠ” ë‚´ìš© í™•ì¸
"""

import asyncio
import aiohttp
import os
from bs4 import BeautifulSoup
from dotenv import load_dotenv

async def debug_html_extraction():
    """ì‹¤ì œ HTML ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ê³¼ì • ë””ë²„ê¹…"""
    # ì„¸ë‹ˆì•„ í´ë¦¬ë‹‰ì˜ ê°œë³„ ìƒí’ˆ í˜ì´ì§€ URL (ë¡œê·¸ì—ì„œ ë°œê²¬ëœ ê²ƒë“¤)
    test_urls = [
        "https://xenia.clinic/ko/products/8a2a54b8-0eaa-4d28-945b-2c76cb98eb9b",
        "https://xenia.clinic/ko/products/bfcad4ed-f697-43bd-9e1a-ab6e16ae6e34",
        "https://xenia.clinic/ko/products/0880d944-4ff6-44f8-8868-924d61668172",
    ]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    async with aiohttp.ClientSession(headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as session:
        for i, url in enumerate(test_urls):
            print(f"\n{'='*80}")
            print(f"ğŸ” URL {i+1}: {url}")
            print('='*80)

            try:
                async with session.get(url) as response:
                    print(f"ğŸ“Š Status Code: {response.status}")
                    print(f"ğŸ“Š Content Type: {response.headers.get('Content-Type', 'N/A')}")

                    if response.status == 200:
                        html_content = await response.text()
                        print(f"ğŸ“Š Raw HTML Length: {len(html_content)} characters")

                        # ì›ë³¸ HTML ìƒ˜í”Œ ì¶œë ¥
                        print(f"\nğŸ“ Raw HTML Sample (first 500 chars):")
                        print("-" * 50)
                        print(html_content[:500])
                        print("-" * 50)

                        # HTML íŒŒì‹± ë° ì „ì²˜ë¦¬ (ì‹¤ì œ LLM extractorì™€ ë™ì¼í•œ ê³¼ì •)
                        soup = BeautifulSoup(html_content, "html.parser")

                        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ì‹¤ì œ ì½”ë“œì™€ ë™ì¼)
                        for tag in soup(["script", "style", "nav", "footer", "header"]):
                            tag.decompose()

                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text_content = soup.get_text(separator=" ", strip=True)
                        print(f"ğŸ“Š Extracted Text Length: {len(text_content)} characters")

                        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì¶œë ¥
                        print(f"\nğŸ“ Extracted Text Sample (first 1000 chars):")
                        print("-" * 50)
                        print(text_content[:1000])
                        print("-" * 50)

                        # ì‹œìˆ  ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
                        treatment_keywords = [
                            'ì‹œìˆ ', 'ì¹˜ë£Œ', 'ì„œë¹„ìŠ¤', 'ê°€ê²©', 'ì›', 'ìƒ·', 'cc', 'ë³´í†¡ìŠ¤', 'í•„ëŸ¬',
                            'ë ˆì´ì €', 'ìŠˆë§í¬', 'ìš¸ì„ë¼', 'ë¦¬í”„íŒ…', 'ì£¼ì‚¬', 'í”„ë¡œê·¸ë¨'
                        ]

                        found_keywords = []
                        for keyword in treatment_keywords:
                            if keyword in text_content:
                                count = text_content.count(keyword)
                                found_keywords.append(f"{keyword}({count}íšŒ)")

                        print(f"\nğŸ” Found Treatment Keywords: {', '.join(found_keywords) if found_keywords else 'ì—†ìŒ'}")

                        # ê°€ê²© íŒ¨í„´ ê²€ìƒ‰
                        import re
                        price_patterns = [
                            r'\d+,?\d+ì›',
                            r'\d+,?\d+\s*ì›',
                            r'â‚©\s*\d+,?\d+',
                            r'\d+ë§Œì›',
                        ]

                        found_prices = []
                        for pattern in price_patterns:
                            matches = re.findall(pattern, text_content)
                            found_prices.extend(matches)

                        print(f"ğŸ’° Found Price Patterns: {found_prices[:10] if found_prices else 'ì—†ìŒ'}")

                        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì²´í¬ (30000ì ì œí•œ)
                        if len(text_content) > 30000:
                            truncated_text = text_content[:30000] + "..."
                            print(f"âš ï¸  Text truncated from {len(text_content)} to 30000 chars")
                        else:
                            truncated_text = text_content
                            print(f"âœ… Text within limit: {len(text_content)} chars")

                        # ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
                        if not truncated_text.strip():
                            print("âŒ CRITICAL: Extracted text is empty!")
                        elif len(truncated_text.strip()) < 100:
                            print(f"âš ï¸  WARNING: Extracted text is very short: {len(truncated_text.strip())} chars")
                        else:
                            print(f"âœ… Extracted text looks good: {len(truncated_text.strip())} chars")

                        # ì¤‘ìš”í•œ êµ¬ì¡° ìš”ì†Œë“¤ í™•ì¸
                        print(f"\nğŸ—ï¸  HTML Structure Analysis:")
                        print(f"- <title>: {soup.title.string if soup.title else 'None'}")
                        print(f"- <h1> tags: {len(soup.find_all('h1'))}")
                        print(f"- <h2> tags: {len(soup.find_all('h2'))}")
                        print(f"- <h3> tags: {len(soup.find_all('h3'))}")
                        print(f"- <p> tags: {len(soup.find_all('p'))}")
                        print(f"- <div> tags: {len(soup.find_all('div'))}")
                        print(f"- <span> tags: {len(soup.find_all('span'))}")

                        # React/SPA ê´€ë ¨ ì²´í¬
                        if 'react' in html_content.lower() or 'vue' in html_content.lower() or 'angular' in html_content.lower():
                            print("âš ï¸  Detected SPA framework - content might be dynamically loaded")

                        if 'window.__INITIAL_STATE__' in html_content or 'window.__PRELOADED_STATE__' in html_content:
                            print("ğŸ“Š Detected initial state data in JavaScript")

                        # JSON-LD êµ¬ì¡°í™”ëœ ë°ì´í„° ì²´í¬
                        json_ld_scripts = soup.find_all('script', type='application/ld+json')
                        if json_ld_scripts:
                            print(f"ğŸ“Š Found {len(json_ld_scripts)} JSON-LD structured data blocks")
                            for j, script in enumerate(json_ld_scripts[:2]):  # ì²˜ìŒ 2ê°œë§Œ í™•ì¸
                                try:
                                    import json
                                    data = json.loads(script.string)
                                    print(f"   JSON-LD {j+1}: {list(data.keys()) if isinstance(data, dict) else type(data)}")
                                except:
                                    print(f"   JSON-LD {j+1}: Failed to parse")

                    else:
                        print(f"âŒ Failed to fetch: HTTP {response.status}")

            except Exception as e:
                print(f"âŒ Error fetching {url}: {str(e)}")

            # ë‹¤ìŒ URL ì²˜ë¦¬ ì „ ì ì‹œ ëŒ€ê¸°
            if i < len(test_urls) - 1:
                await asyncio.sleep(2)

if __name__ == "__main__":
    asyncio.run(debug_html_extraction())